{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 1: Import Libraries and definitions\n",
    "import numpy as np\n",
    "\n",
    "class Tensor (object):\n",
    "    \n",
    "    def __init__(self,data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.autograd = autograd\n",
    "        self.grad = None\n",
    "        if(id is None):\n",
    "            self.id = np.random.randint(0,100000)\n",
    "        else:\n",
    "            self.id = id\n",
    "        \n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.children = {}\n",
    "        \n",
    "        if(creators is not None):\n",
    "            for c in creators:\n",
    "                if(self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for id,cnt in self.children.items():\n",
    "            if(cnt != 0):\n",
    "                return False\n",
    "        return True \n",
    "        \n",
    "    def backward(self,grad=None, grad_origin=None):\n",
    "        if(self.autograd):\n",
    " \n",
    "            if(grad is None):\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "\n",
    "            if(grad_origin is not None):\n",
    "                if(self.children[grad_origin.id] == 0):\n",
    "                    raise Exception(\"cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "            if(self.grad is None):\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "            \n",
    "            # grads must not have grads of their own\n",
    "            assert grad.autograd == False\n",
    "            \n",
    "            # only continue backpropping if there's something to\n",
    "            # backprop into and if all gradients (from children)\n",
    "            # are accounted for override waiting for children if\n",
    "            # \"backprop\" was called on this variable directly\n",
    "            if(self.creators is not None and \n",
    "               (self.all_children_grads_accounted_for() or \n",
    "                grad_origin is None)):\n",
    "\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                    \n",
    "                if(self.creation_op == \"sub\"):\n",
    "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
    "                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
    "\n",
    "                if(self.creation_op == \"mul\"):\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new , self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)                    \n",
    "                    \n",
    "                if(self.creation_op == \"mm\"):\n",
    "                    c0 = self.creators[0]\n",
    "                    c1 = self.creators[1]\n",
    "                    new = self.grad.mm(c1.transpose())\n",
    "                    c0.backward(new)\n",
    "                    new = self.grad.transpose().mm(c0).transpose()\n",
    "                    c1.backward(new)\n",
    "                    \n",
    "                if(self.creation_op == \"transpose\"):\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "\n",
    "                if(\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.expand(dim,\n",
    "                                                               self.creators[0].data.shape[dim]))\n",
    "\n",
    "                if(\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "                    \n",
    "                if(self.creation_op == \"neg\"):\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                    \n",
    "                if(self.creation_op == \"sigmoid\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
    "                \n",
    "                if(self.creation_op == \"tanh\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
    "                \n",
    "                if(self.creation_op == \"index_select\"):\n",
    "                    new_grad = np.zeros_like(self.creators[0].data)\n",
    "                    indices_ = self.index_select_indices.data.flatten()\n",
    "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
    "                    for i in range(len(indices_)):\n",
    "                        new_grad[indices_[i]] += grad_[i]\n",
    "                    self.creators[0].backward(Tensor(new_grad))\n",
    "                    \n",
    "                if(self.creation_op == \"cross_entropy\"):\n",
    "                    dx = self.softmax_output - self.target_dist\n",
    "                    self.creators[0].backward(Tensor(dx))\n",
    "                    \n",
    "    def __add__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __neg__(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data * -1,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"neg\")\n",
    "        return Tensor(self.data * -1)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)    \n",
    "\n",
    "    def sum(self, dim):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.sum(dim),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sum_\"+str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "    \n",
    "    def expand(self, dim,copies):\n",
    "\n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        trans_cmd.insert(dim,len(self.data.shape))\n",
    "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
    "        \n",
    "        if(self.autograd):\n",
    "            return Tensor(new_data,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"expand_\"+str(dim))\n",
    "        return Tensor(new_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.transpose(),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"transpose\")\n",
    "        \n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.dot(x.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self,x],\n",
    "                          creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(1 / (1 + np.exp(-self.data)),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sigmoid\")\n",
    "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
    "\n",
    "    def tanh(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(np.tanh(self.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"tanh\")\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    def index_select(self, indices):\n",
    "\n",
    "        if(self.autograd):\n",
    "            new = Tensor(self.data[indices.data],\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"index_select\")\n",
    "            new.index_select_indices = indices\n",
    "            return new\n",
    "        return Tensor(self.data[indices.data])\n",
    "\n",
    "    def softmax(self):\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp,\n",
    "                                       axis=len(self.data.shape)-1,\n",
    "                                       keepdims=True)\n",
    "        return softmax_output    \n",
    "    \n",
    "    def cross_entropy(self, target_indices):\n",
    "\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp,\n",
    "                                       axis=len(self.data.shape)-1,\n",
    "                                       keepdims=True)\n",
    "        \n",
    "        t = target_indices.data.flatten()\n",
    "        p = softmax_output.reshape(len(t),-1)\n",
    "        target_dist = np.eye(p.shape[1])[t]\n",
    "        loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
    "    \n",
    "        if(self.autograd):\n",
    "            out = Tensor(loss,\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"cross_entropy\")\n",
    "            out.softmax_output = softmax_output\n",
    "            out.target_dist = target_dist\n",
    "            return out\n",
    "\n",
    "        return Tensor(loss)\n",
    "        \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())  \n",
    "\n",
    "class Layer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.parameters = list()\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        return self.parameters\n",
    "    \n",
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "    \n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()\n",
    "    \n",
    "class SGD(object):\n",
    "    \n",
    "    def __init__(self, parameters, alpha=0.1):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "        \n",
    "    def step(self, zero=True):\n",
    "        \n",
    "        for p in self.parameters:\n",
    "            \n",
    "            p.data -= p.grad.data * self.alpha\n",
    "            \n",
    "            if(zero):\n",
    "                p.grad.data *= 0\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super().__init__()\n",
    "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))\n",
    "        self.weight = Tensor(W, autograd=True)\n",
    "        self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
    "        \n",
    "        self.parameters.append(self.weight)\n",
    "        self.parameters.append(self.bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.mm(self.weight)+self.bias.expand(0,len(input.data))\n",
    "\n",
    "\n",
    "class Sequential(Layer):\n",
    "    \n",
    "    def __init__(self, layers=list()):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = layers\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        params = list()\n",
    "        for l in self.layers:\n",
    "            params += l.get_parameters()\n",
    "        return params\n",
    "\n",
    "\n",
    "class Embedding(Layer):\n",
    "    \n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        \n",
    "        # this random initialiation style is just a convention from word2vec\n",
    "        self.weight = Tensor((np.random.rand(vocab_size, dim) - 0.5) / dim, autograd=True)\n",
    "        \n",
    "        self.parameters.append(self.weight)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.weight.index_select(input)\n",
    "\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()\n",
    "    \n",
    "\n",
    "class CrossEntropyLoss(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        return input.cross_entropy(target)\n",
    "\n",
    "    \n",
    "class RNNCell(Layer):\n",
    "    \n",
    "    def __init__(self, n_inputs, n_hidden, n_output, activation='sigmoid'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        if(activation == 'sigmoid'):\n",
    "            self.activation = Sigmoid()\n",
    "        elif(activation == 'tanh'):\n",
    "            self.activation == Tanh()\n",
    "        else:\n",
    "            raise Exception(\"Non-linearity not found\")\n",
    "\n",
    "        self.w_ih = Linear(n_inputs, n_hidden)\n",
    "        self.w_hh = Linear(n_hidden, n_hidden)\n",
    "        self.w_ho = Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.parameters += self.w_ih.get_parameters()\n",
    "        self.parameters += self.w_hh.get_parameters()\n",
    "        self.parameters += self.w_ho.get_parameters()        \n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        from_prev_hidden = self.w_hh.forward(hidden)\n",
    "        combined = self.w_ih.forward(input) + from_prev_hidden\n",
    "        new_hidden = self.activation.forward(combined)\n",
    "        output = self.w_ho.forward(new_hidden)\n",
    "        return output, new_hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNN Character Language Model\n",
    "import sys,random,math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "np.random.seed(0)\n",
    "f = open('tinyshakespeare.txt','r')\n",
    "raw = f.read()\n",
    "f.close()\n",
    "\n",
    "vocab = list(set(raw))\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "indices = np.array(list(map(lambda x:word2index[x], raw)))\n",
    "\n",
    "embed = Embedding(vocab_size=len(vocab),dim=512)\n",
    "model = RNNCell(n_inputs=512, n_hidden=512, n_output=len(vocab))\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optim = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)\n",
    "\n",
    "batch_size = 32\n",
    "bptt = 16\n",
    "n_batches = int((indices.shape[0] / (batch_size)))\n",
    "\n",
    "trimmed_indices = indices[:n_batches*batch_size]\n",
    "batched_indices = trimmed_indices.reshape(batch_size, n_batches).transpose()\n",
    "\n",
    "input_batched_indices = batched_indices[0:-1]\n",
    "target_batched_indices = batched_indices[1:]\n",
    "\n",
    "n_bptt = int(((n_batches-1) / bptt))\n",
    "input_batches = input_batched_indices[:n_bptt*bptt].reshape(n_bptt,bptt,batch_size)\n",
    "target_batches = target_batched_indices[:n_bptt*bptt].reshape(n_bptt, bptt, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 17, 21,  2, 11])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19, 45, 53, 38, 51, 15, 53, 34, 53, 53,  2, 12, 26, 63, 34, 34,\n",
       "        24, 24, 34, 24,  1, 26, 26, 53, 26, 24, 24,  6, 44, 24, 25,  1],\n",
       "       [17, 34, 22, 34, 23, 41, 56, 44, 38, 45, 24, 34, 23, 49, 56,  3,\n",
       "        53, 43, 35, 35, 11, 26,  2, 56, 26, 43, 46, 21, 16, 24, 53, 16],\n",
       "       [21, 21, 34,  2, 53, 33,  1, 53, 34, 34,  1, 21, 63, 38, 27, 51,\n",
       "        45, 23, 23, 24, 53, 53, 53, 34, 53, 63, 53, 34, 38, 53, 45, 24],\n",
       "       [ 2, 53, 21, 24, 34,  7, 35, 12, 12, 34, 23,  2, 58,  1, 17, 28,\n",
       "        26, 53, 53, 56, 17,  3, 27, 21, 34,  6, 17, 42, 53, 27, 26, 21],\n",
       "       [11, 41, 17, 53, 21,  9, 53, 24, 53, 11, 63, 24, 34, 11, 35, 53,\n",
       "         1, 45, 45, 25,  2, 34, 24, 24, 35, 24, 45, 34, 25, 24,  1, 23]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_indices[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19, 45, 53, 38, 51, 15, 53, 34, 53, 53,  2, 12, 26, 63, 34, 34,\n",
       "        24, 24, 34, 24,  1, 26, 26, 53, 26, 24, 24,  6, 44, 24, 25,  1],\n",
       "       [17, 34, 22, 34, 23, 41, 56, 44, 38, 45, 24, 34, 23, 49, 56,  3,\n",
       "        53, 43, 35, 35, 11, 26,  2, 56, 26, 43, 46, 21, 16, 24, 53, 16],\n",
       "       [21, 21, 34,  2, 53, 33,  1, 53, 34, 34,  1, 21, 63, 38, 27, 51,\n",
       "        45, 23, 23, 24, 53, 53, 53, 34, 53, 63, 53, 34, 38, 53, 45, 24],\n",
       "       [ 2, 53, 21, 24, 34,  7, 35, 12, 12, 34, 23,  2, 58,  1, 17, 28,\n",
       "        26, 53, 53, 56, 17,  3, 27, 21, 34,  6, 17, 42, 53, 27, 26, 21],\n",
       "       [11, 41, 17, 53, 21,  9, 53, 24, 53, 11, 63, 24, 34, 11, 35, 53,\n",
       "         1, 45, 45, 25,  2, 34, 24, 24, 35, 24, 45, 34, 25, 24,  1, 23]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batches[0][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17, 34, 22, 34, 23, 41, 56, 44, 38, 45, 24, 34, 23, 49, 56,  3,\n",
       "        53, 43, 35, 35, 11, 26,  2, 56, 26, 43, 46, 21, 16, 24, 53, 16],\n",
       "       [21, 21, 34,  2, 53, 33,  1, 53, 34, 34,  1, 21, 63, 38, 27, 51,\n",
       "        45, 23, 23, 24, 53, 53, 53, 34, 53, 63, 53, 34, 38, 53, 45, 24],\n",
       "       [ 2, 53, 21, 24, 34,  7, 35, 12, 12, 34, 23,  2, 58,  1, 17, 28,\n",
       "        26, 53, 53, 56, 17,  3, 27, 21, 34,  6, 17, 42, 53, 27, 26, 21],\n",
       "       [11, 41, 17, 53, 21,  9, 53, 24, 53, 11, 63, 24, 34, 11, 35, 53,\n",
       "         1, 45, 45, 25,  2, 34, 24, 24, 35, 24, 45, 34, 25, 24,  1, 23],\n",
       "       [53, 34, 34, 38, 53, 63, 12, 21, 11, 46, 49, 53, 53, 53, 24, 49,\n",
       "        11,  1,  1, 23, 53, 35, 53, 53, 24, 21, 53,  2, 34, 26, 11, 53]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_batches[0][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the RNN Character Language Model\n",
    "def generate_sample(n=30, init_char=' '):\n",
    "    s = \"\"\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    input = Tensor(np.array([word2index[init_char]]))\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(input)\n",
    "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "        output.data *= 10\n",
    "        temp_dist = output.softmax()\n",
    "        temp_dist /= temp_dist.sum()\n",
    "\n",
    "        m = (temp_dist > np.random.rand()).argmax()\n",
    "#         m = output.data.argmax()\n",
    "        c = vocab[m]\n",
    "        input = Tensor(np.array([m]))\n",
    "        s += c\n",
    "    return s\n",
    "\n",
    "\n",
    "def train(iterations=400):\n",
    "    for iter in range(iterations):\n",
    "        total_loss = 0\n",
    "        n_loss = 0\n",
    "\n",
    "        hidden = model.init_hidden(batch_size=batch_size)\n",
    "        for batch_i in range(len(input_batches)):\n",
    "\n",
    "            hidden = Tensor(hidden.data, autograd=True)\n",
    "            loss = None\n",
    "            losses = list()\n",
    "            for t in range(bptt):\n",
    "                input = Tensor(input_batches[batch_i][t], autograd=True)\n",
    "                rnn_input = embed.forward(input=input)\n",
    "                output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "\n",
    "                target = Tensor(target_batches[batch_i][t], autograd=True)    \n",
    "                batch_loss = criterion.forward(output, target)\n",
    "                losses.append(batch_loss)\n",
    "                if(t == 0):\n",
    "                    loss = batch_loss\n",
    "                else:\n",
    "                    loss = loss + batch_loss\n",
    "            for loss in losses:\n",
    "                \"\"\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total_loss += loss.data\n",
    "            log = \"\\r Iter:\" + str(iter)\n",
    "            log += \" - Batch \"+str(batch_i+1)+\"/\"+str(len(input_batches))\n",
    "            log += \" - Loss:\" + str(np.exp(total_loss / (batch_i+1)))\n",
    "            if(batch_i == 0):\n",
    "                log += \" - \" + generate_sample(n=70, init_char='\\n').replace(\"\\n\",\" \")\n",
    "            if(batch_i % 10 == 0 or batch_i-1 == len(input_batches)):\n",
    "                sys.stdout.write(log)\n",
    "        optim.alpha *= 0.99\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter:0 - Batch 2171/2178 - Loss:13.679778157132636rrrrrrrrrrKrrrrrrrrKrrrrrrrrrrrrrKrrrrrrsrrrrrrrrrrKrrrrrrrrrrrKrrrrrr\n",
      " Iter:1 - Batch 2171/2178 - Loss:8.892382501962324 te and and and and and and and and and and and and and and and and and\n",
      " Iter:2 - Batch 2171/2178 - Loss:7.8578605422068355ear the sKeath and and and and and and and and and and Kester and Ke \n",
      " Iter:3 - Batch 2171/2178 - Loss:7.2508269444820065eave and and and and and and KKer and and and and and Kaster and and \n",
      " Iter:4 - Batch 2171/2178 - Loss:6.8168601843866695I have and and Kanges and and Kanges and Kanges and and and Kanges an\n",
      " Iter:5 - Batch 2171/2178 - Loss:6.4761416194282075hath the son the son the son the son the son the son the son the sKe \n",
      " Iter:6 - Batch 2171/2178 - Loss:6.1998167341682636haKe Kanges and the son the son the son the son the son the son the s\n",
      " Iter:7 - Batch 2171/2178 - Loss:5.9658741713490475onger and the seept KKesK the sKe and the sKe Kanges and Kanges and t\n",
      " Iter:8 - Batch 2171/2178 - Loss:5.7649124495138735Const KenKes and the seeKe the sKKens and KangesK the seepKes and the \n",
      " Iter:9 - Batch 2171/2178 - Loss:5.5884991408659595Kest and the seepKeKents and the sKKens and sKeet of the seepKes and \n",
      " Iter:10 - Batch 2171/2178 - Loss:5.4285745563890745onst of the sKall Kate Kall the see and the see and Kate Kall the ste\n",
      " Iter:11 - Batch 2171/2178 - Loss:5.2849220101556655onstKens and the sKing and the ste are and the ste KaKe Kangess and t\n",
      " Iter:12 - Batch 2171/2178 - Loss:5.1544767378462835onswer and Kate and the ste are Kate and Kangess and the steKe Kanges\n",
      " Iter:13 - Batch 2171/2178 - Loss:5.0323358507845895onswer and KaKens and the steKe Kangess Kangess Kangess and the sKing\n",
      " Iter:14 - Batch 2171/2178 - Loss:4.9175992951715255Conswer Kangess Kangess Kangess and Kangess Kangess and Kangess Kanges\n",
      " Iter:15 - Batch 2171/2178 - Loss:4.8083063455171835ATERLANUS: All Kangess Kangess and Kate KangeKs and steKp and Kate Ka\n",
      " Iter:16 - Batch 2171/2178 - Loss:4.7057599923498035KATHARD IV: All his haKe the sKingly and Kate and the sKingKens and Ka\n",
      " Iter:17 - Batch 2171/2178 - Loss:4.6075463857914175hose and Kate Kate the shall be shall Kate the shall Kate the steKn t\n",
      " Iter:18 - Batch 2171/2178 - Loss:4.5091877128225595hose KaKens and see sKKeKs and ste shall Kate the shall be sKords and\n",
      " Iter:19 - Batch 2171/2178 - Loss:4.4112822226662225hose and Kate the shall be sKords and seKen: She have see is a ste ar\n",
      " Iter:20 - Batch 2171/2178 - Loss:4.3174700080809214host and the shall be sound.  Second and Kate the KKnguise: Know not \n",
      " Iter:21 - Batch 2171/2178 - Loss:4.2288069273037395osKl his Kased, and Kate the shall be so his Kased and Kate the KKng\n",
      " Iter:22 - Batch 2171/2178 - Loss:4.1417784785294955osKl his horse and Kate, and see save the seese and and and the sKKe\n",
      " Iter:23 - Batch 2171/2178 - Loss:4.0633711212679335host answer and and Kate the King, and Kate the shall before the shal\n",
      " Iter:24 - Batch 2171/2178 - Loss:3.9812954436097985hose forgow, and all see in the King, and and Kate the sKall the seal\n",
      " Iter:25 - Batch 2171/2178 - Loss:3.9094888712769937All and all senter and all senter and KateKn, and all senter and all \n",
      " Iter:26 - Batch 2171/2178 - Loss:3.8356632257856433ars and all seKen: Should be a senses and KateKs Kate his honourKs, \n",
      " Iter:27 - Batch 2171/2178 - Loss:3.7645072892678977Kel san, and Kate the shall be not and all sensest and Kate the King \n",
      " Iter:28 - Batch 2171/2178 - Loss:3.7078289503532475irst allow not and all sensest an Kate his best reath is a senter Kin\n",
      " Iter:29 - Batch 2171/2178 - Loss:3.6446329924729715earing KingKentKs and KateKKness is stern, And sKiKns King of empty a\n",
      " Iter:30 - Batch 2171/2178 - Loss:3.5646056500987985eKll's KingK, HerK, an ember hardly are KingK! How not and the shall \n",
      " Iter:31 - Batch 2171/2178 - Loss:3.5062518590828216ars King of eKs' words and see his both is shall be KateKn, and seKe\n",
      " Iter:32 - Batch 2171/2178 - Loss:3.4844718478587784Men him King of emp, and sKords and King of empKs the more the King of\n",
      " Iter:33 - Batch 2171/2178 - Loss:3.4354750153409977een and so King of respected and so not stentrer were is KingK! The h\n",
      " Iter:34 - Batch 2171/2178 - Loss:3.3544354380215317eKal the King; and sKing of an already and KinglK: All stemply and se\n",
      " Iter:35 - Batch 2171/2178 - Loss:3.2988463865073783earKst best carriage of King of your hoKe: Kends the sKall hKing of e\n",
      " Iter:36 - Batch 2171/2178 - Loss:3.2727960481405765en hake graves doubts is sweet of emps and less see shall be not bear\n",
      " Iter:37 - Batch 2171/2178 - Loss:3.2627056847824528endy and send we haste King of King of the King King of rest King Kin\n",
      " Iter:38 - Batch 2171/2178 - Loss:3.2083850767748883MKene is sKKCless seKens.  ROMEO: Alrecies, and the shall be should Ki\n",
      " Iter:39 - Batch 2171/2178 - Loss:3.1259079294367655Men: Sury, the shall hath not and alresh hKind and the KingK! Hor esK \n",
      " Iter:40 - Batch 2171/2178 - Loss:3.0957343048361022n: Sir, and how not and send and sense: and so ing or hath King Ling\n",
      " Iter:41 - Batch 2171/2178 - Loss:3.0797702044810022Mend; and the King; and sense: AKing sense, and Kate the King; and the\n",
      " Iter:42 - Batch 2171/2178 - Loss:3.0478669991440046agis not and alr, and send and send and send and Kase is seeK'd hands\n",
      " Iter:43 - Batch 2171/2178 - Loss:3.0253848537021427ndoking armward areKe. What, and Kord and Kord and Kates, and the ple\n",
      " Iter:44 - Batch 2171/2178 - Loss:3.0163612434825375entry the King, All and seeing the King, All and seeing Kingl words, \n",
      " Iter:45 - Batch 2171/2178 - Loss:2.9640961243639414en and the Kinglord and seeing. GenKing are forgoh: I am not a harK s\n",
      " Iter:46 - Batch 2171/2178 - Loss:2.9537525764911647Mentress the shrish, and the King; and the shriKks you are thou Kaster\n",
      " Iter:47 - Batch 2171/2178 - Loss:2.8992775017490007est, and sorrow so King of ther, and sorrow men Kard bKInd and seeing\n",
      " Iter:48 - Batch 2171/2178 - Loss:2.9110129567761263busin to be the sharm die so not a love an alo Knal speak, have plage\n",
      " Iter:49 - Batch 2171/2178 - Loss:2.8389828916572566nd are and the eased and the name is a hardy and Kondyart and the wor\n",
      " Iter:50 - Batch 2171/2178 - Loss:2.8054555475819662an. Comf, and the plagen, and seeing to Kings: I am not a KIn: but se\n",
      " Iter:51 - Batch 2171/2178 - Loss:2.8309342786221436K:en: Slave KaKensining the name; and the King and the King King Lord \n",
      " Iter:52 - Batch 2171/2178 - Loss:2.8165811992239402Man. Fear the words, and the words, And Kate the words, and the words,\n",
      " Iter:53 - Batch 2171/2178 - Loss:2.7440533073011975Mbreat-- Thought and Ky werplangly and the King, And seKin set down an\n",
      " Iter:54 - Batch 2171/2178 - Loss:2.7057575651403745t iK no seem a steak, Kate the world; And seeinK: Have years, and see\n",
      " Iter:55 - Batch 2171/2178 - Loss:2.7232462494377683nd King Lord Henry! I would sKendK then, and the words, and the words\n",
      " Iter:56 - Batch 2171/2178 - Loss:2.6760003940225077and Geoth of destal to his far in a breaves the shrifted King and the \n",
      " Iter:57 - Batch 2171/2178 - Loss:2.6698784075482635nd as should be full of men:KKen, and the Kind and my brother, and th\n",
      " Iter:58 - Batch 2171/2178 - Loss:2.6404099288049245sentK Mottensing of an all seemend, And stemplesise, stemples his fac\n",
      " Iter:59 - Batch 2171/2178 - Loss:2.6562180789916954toKen and Kong of Ellows and Kond me for on myKensin, we pKeen King.  \n",
      " Iter:60 - Batch 2171/2178 - Loss:2.6387250764200414and as speak shall Kate forgove answer the words, And sees and Kook'd:\n",
      " Iter:61 - Batch 2171/2178 - Loss:2.6292134750875022The she and KaKing seeKs the world; And say shall be put in the shKIlw\n",
      " Iter:62 - Batch 2171/2178 - Loss:2.6133482932082983nd. She stone before the she not rean too erK-light Kate farth the sh\n",
      " Iter:63 - Batch 2171/2178 - Loss:2.5907834088852404IndiKn to seementable of words, Whose forget, Kate the King and the sh\n",
      " Iter:64 - Batch 2171/2178 - Loss:2.5423804831819568weK'se forgo Willing him on,King my theK. Dess the side forgo Willing\n",
      " Iter:65 - Batch 2171/2178 - Loss:2.5316665085812353nape, himse; then rest in a fire the  the  the  the  the  the  the  t\n",
      " Iter:66 - Batch 2171/2178 - Loss:2.5236278485453747Inspless Kate far of Now, as Kase I see the King EdWARD IV: Kell sees \n",
      " Iter:67 - Batch 2171/2178 - Loss:2.5458211620317748nd. What I see the plage.  Secons and theKs and they are all the word\n",
      " Iter:68 - Batch 2171/2178 - Loss:2.5299450007402387Ind. The sKord before the widow lies hath reKent.  CORIOLANUS: I neKiK\n",
      " Iter:69 - Batch 2171/2178 - Loss:2.4520455641665566In plainly and King and toward befies aKot, and then, go beg on our si\n",
      " Iter:70 - Batch 2171/2178 - Loss:2.4414821677060288nape, her work lorger, her ano we have a KIts King and on the shall b\n",
      " Iter:71 - Batch 2171/2178 - Loss:2.4384544278186767Inaping are are a seeK, and love and on the words are are a seesyn? pl\n",
      " Iter:72 - Batch 2171/2178 - Loss:2.4144934295261413d; And say say, and let me from a hards arm!  S: All in a bran.  ROM\n",
      " Iter:73 - Batch 2171/2178 - Loss:2.4010927339706237n proof my the world; And say say, the wind, and then,--Kate is a far\n",
      " Iter:74 - Batch 2171/2178 - Loss:2.3703687399732467s not him words, and the sea, all the King Her,Ker hang 'Ke out, sir,\n",
      " Iter:75 - Batch 2171/2178 - Loss:2.3959177329413834nape, hence! Kind the widiKing him to her we have plage.  First Mursh\n",
      " Iter:76 - Batch 2171/2178 - Loss:2.3845635229403297Inape, him courtely in King HereKen for the world; And sented, as the \n",
      " Iter:77 - Batch 2171/2178 - Loss:2.3733950586508137Inap ens: the sister, and reman good love and King as speak shall sent\n",
      " Iter:78 - Batch 2171/2178 - Loss:2.3729870525742134Inap, and the winters and sent of him to the shall sentless meKens; Sh\n",
      " Iter:79 - Batch 2171/2178 - Loss:2.3765993283033726nape, her would be south. Al, the King and where is heK. The eing him\n",
      " Iter:80 - Batch 2171/2178 - Loss:2.3369900190186943s not him for the wilth are.  Second Gentle good to have plage.  ROME\n",
      " Iter:81 - Batch 2171/2178 - Loss:2.3234906649688374Is nours.  ROMEO: KKef royalous steen use or be are King KInd.  Second\n",
      " Iter:82 - Batch 2171/2178 - Loss:2.3307892249555096s nonKslous for the King King Here's sonKs heKs Kinglord.  CORIOLANUS\n",
      " Iter:83 - Batch 2171/2178 - Loss:2.2979420856596193n Kingly. How and were and we havs against a hard, thereof Kine and w\n",
      " Iter:84 - Batch 2171/2178 - Loss:2.2690386005885646In prother.  Roman wKrn but vill-dare:  KING RICHARD II: My son: Ste k\n",
      " Iter:85 - Batch 2171/2178 - Loss:2.2942345014079204n proof with the words, And seeK And be our soldiers in the words, An\n",
      " Iter:86 - Batch 2171/2178 - Loss:2.2049645841168166Inap and besem, in the pain. KnKelf dispaint George our eyes, finds th\n",
      " Iter:87 - Batch 2171/2178 - Loss:2.2015519340989025Inape, hence!  DUKE OF YORK: So, beseKs! I schide to King Edward.  ROM\n",
      " Iter:88 - Batch 2171/2178 - Loss:2.1955277617996463Is not hKind; best of hiKe.  PETRUCHIO: Katch harry, to arms the wife,\n",
      " Iter:89 - Batch 2171/2178 - Loss:2.2079324445517274naping grew. AnKENVOLICKINGHAM: My fair ear, as if would then, yet re\n",
      " Iter:90 - Batch 2171/2178 - Loss:2.1861323760442266Inap, and then rude and sees and sees and seeK And seKs were courteKan\n",
      " Iter:91 - Batch 2171/2178 - Loss:2.1578651497641204Inspent go would be dark us is sKell.  AUTOLYCUS: I do aloKd and would\n",
      " Iter:92 - Batch 2171/2178 - Loss:2.1822392961380603Inlish would stanich Dom clouds hath see well. When courteen Kate her \n",
      " Iter:93 - Batch 2171/2178 - Loss:2.2110840503489215Ins thy hands.  ROMEO: And, Ketter had not rue not hath Kate's and wor\n",
      " Iter:94 - Batch 2171/2178 - Loss:2.1484686114612874Inspent of prepare A sKendst our landed say in the pluckspel. The 's i\n",
      " Iter:95 - Batch 2171/2178 - Loss:2.1097707568876397till.  CORIOLANUS: Grumise; I will be plucking can any the will weekK\n",
      " Iter:96 - Batch 2171/2178 - Loss:2.0961150595857347IKsKing and what so soonKe, over hence.  KING EDWARD IV: MOO We son,Kp\n",
      " Iter:97 - Batch 2171/2178 - Loss:2.0681483297817467Is not him well him cust grKemory to his face on thKing awfKpt; And sK\n",
      " Iter:98 - Batch 2171/2178 - Loss:2.0859015713722875Insplingly and where ther, and then, a verly and sender where to be on\n",
      " Iter:99 - Batch 2171/2178 - Loss:2.0940086658534494s any words, When his faceK? dement in the ropplKs.  ROMEO: I see him\n"
     ]
    }
   ],
   "source": [
    "train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter:0 - Batch 2171/2178 - Loss:2.1039810902791727Inapen are worthy celve so the rops of King Lord Aument. Montague, we \n",
      " Iter:1 - Batch 2171/2178 - Loss:2.0437529045550042Inaping all and what is is swive a his to deputed, will be deliest wit\n",
      " Iter:2 - Batch 2171/2178 - Loss:2.0458862462458884Inampering the Thou wise forbiunanceKst of reliem.  SHO: And yet him c\n",
      " Iter:3 - Batch 2171/2178 - Loss:2.0143522037214693aping to messebing breat and words, And say so, the shristing oKent.\n",
      " Iter:4 - Batch 2171/2178 - Loss:1.9976982033643924inderenK.  So bosom.  SAMPETRUCHIO: Make To be himsed him chamber and\n",
      " Iter:5 - Batch 2171/2178 - Loss:2.0038665881786057Inam to his hours. For on my trory carshappy hath do this nupersing of\n",
      " Iter:6 - Batch 2171/2178 - Loss:1.9927188355680847Is the King Liry but one king'st of men: but I would should and what K\n",
      " Iter:7 - Batch 2171/2178 - Loss:1.9982386546664002nape to her sir, and my lord; there there?  POLINCE: The sition, Whic\n",
      " Iter:8 - Batch 2171/2178 - Loss:1.9971924084917858Inap, and then rude and words, Katter, and then rude and words, Katter\n",
      " Iter:9 - Batch 2171/2178 - Loss:1.9967456403853982nap, and then rusonter which seeK DI truK Her: In paint Gerant theirs\n",
      " Iter:10 - Batch 2171/2178 - Loss:2.0219513917280167Is now, pencit Which she not him Kate counswer.  PETRUCHIO: Katch him \n",
      " Iter:11 - Batch 2171/2178 - Loss:2.0056343870539455Inap, Kate I see the counsels: I would be ill. Ter and with thy lord. \n",
      " Iter:12 - Batch 2171/2178 - Loss:1.9417512316413068Is the King I sent ing it king; I Kind oKK And stale death, as a tende\n",
      " Iter:13 - Batch 2171/2178 - Loss:1.9104441855808258s the shrese Krocle, or were look of any what so me, Kate is death th\n",
      " Iter:14 - Batch 2171/2178 - Loss:1.9507229347734296Is the name and tKengureK. And so wife, my lord, that I shall be disca\n",
      " Iter:15 - Batch 2171/2178 - Loss:1.9216240249270828Manish'd before slave good lord before slave good loKence.  CORIOLANUS\n",
      " Iter:16 - Batch 2171/2178 - Loss:1.8785823535615662Is the words thou wert in the world; And sweek of bloody to our senter\n",
      " Iter:17 - Batch 2171/2178 - Loss:1.8765889995858078In prospering him change over be ble. I deliver gentleman.  ROMEO: Lov\n",
      " Iter:18 - Batch 2171/2178 - Loss:1.8463080204424338IKING EDWARD IV: Seconarish'd breath is leavess to his Kate to hKill a\n",
      " Iter:19 - Batch 2171/2178 - Loss:1.8522383629481622Ishard prove-mis-baKks theK. Yes, conKundst the windosK there?  BRUTUS\n",
      " Iter:20 - Batch 2171/2178 - Loss:1.8404570276410148Inap reaning she Thing is s,ace oatKs! You have preumian?  KING EDWARD\n",
      " Iter:21 - Batch 2171/2178 - Loss:1.8439343640609531Is not him well-dirent him chaKen but Kinglont their advide our laude.\n",
      " Iter:22 - Batch 2171/2178 - Loss:1.8464577022125312Inap restKent dreads and words, And so so a Kate in the loacesKate of \n",
      " Iter:23 - Batch 2171/2178 - Loss:1.7864946407964795nspoth the words, And sir: but I am not. Sir, and there be our dished\n",
      " Iter:24 - Batch 2171/2178 - Loss:1.7632710391208482Inapes it our screaturast.  FLORIZCLADY ANNE: Why, sir, who wise and s\n",
      " Iter:25 - Batch 2171/2178 - Loss:1.7814526693490162Is the sighness.  SLY: AKing rook'd him cuttory call post dKING RICHAR\n",
      " Iter:26 - Batch 2171/2178 - Loss:1.7916939112319783Inade, Kore forgo with Kindloss did say so, and the winK.  KING EDWARD\n",
      " Iter:27 - Batch 2171/2178 - Loss:1.7496151481127455nap, for a hirmentsK: WhKing what shen posside in the windose their b\n",
      " Iter:28 - Batch 2171/2178 - Loss:1.7626289170371734Isom a suitorK; And then rudness with her wordsKing speak.  YORK: How \n",
      " Iter:29 - Batch 2171/2178 - Loss:1.7367202876113217Inape, KoKe good awhilio: Sir, where letter.  KING RICHARD II: Be souK\n",
      " Iter:30 - Batch 2171/2178 - Loss:1.7460118020947892Is the rought me from and which she KeKentainty loas are but keep to h\n",
      " Iter:31 - Batch 2171/2178 - Loss:1.7197232470737693Kase; and then redeed her words, Which wordsKer specting Kate is bKin\n",
      " Iter:32 - Batch 2171/2178 - Loss:1.6950495574340683Ison away: Witter him cuttome and then redeed.  SICINIUS: You are on t\n",
      " Iter:33 - Batch 2171/2178 - Loss:1.6842237978356907s the king at spect with Kind him confeKing then, Kome before his fai\n",
      " Iter:34 - Batch 2171/2178 - Loss:1.6889807810689006son. Then reKried of receKrts Kind Henry, beseKoth King Heres, while \n",
      " Iter:35 - Batch 2171/2178 - Loss:1.6691670517752082Is a bidness of a prisonmemence is deser his face of would be did spul\n",
      " Iter:36 - Batch 2171/2178 - Loss:1.6496899226173047Is the kingd theK in the KinK, the coverly.  ROMEO: And, I will be the\n",
      " Iter:37 - Batch 2171/2178 - Loss:1.6433012139327837Is the wixtiance; and, kingly shall we do thKe strange on his hath pai\n",
      " Iter:38 - Batch 2171/2178 - Loss:1.6222666284776117IKING ESCANTIO: Why, some dayse; Kind upon him.  RICHARD III: End, as \n",
      " Iter:39 - Batch 2171/2178 - Loss:1.6149578192556808It corrow. Sir, where they hade, I will be decaurst is in the wides th\n",
      " Iter:40 - Batch 2171/2178 - Loss:1.5964771640383941and his hath be confire to meew, that straight deling Kate courield an\n",
      " Iter:41 - Batch 2171/2178 - Loss:1.5630512780596628Is the suital, and then rudeged ber than his father, yet up thy sovere\n",
      " Iter:42 - Batch 2171/2178 - Loss:1.5684559766843478Is the suitted him well.K  MENENIUS: There leaves, and there Where Kin\n",
      " Iter:43 - Batch 2171/2178 - Loss:1.5639778485393216and him well have prevenge it so from and seeing to do this advisise, \n",
      " Iter:44 - Batch 2171/2178 - Loss:1.5280671015159628anatious from a suf, and my swift-daKing him cuttorious Kind of rast, \n",
      " Iter:45 - Batch 2171/2178 - Loss:1.5229103153276171Is the shristKents and what has not hath shamely you shall I am not. S\n",
      " Iter:46 - Batch 2171/2178 - Loss:1.5434370315275654natious King of what I Kind Kate of KIte's shall we confKr a partal t\n",
      " Iter:47 - Batch 2171/2178 - Loss:1.5331883836144247Is the merbre them Kingl God, hence, and the windows with the sun, The\n",
      " Iter:48 - Batch 2171/2178 - Loss:1.5255737399967735Ison the way to his face of any one King Henry! Kind HalKI, you Kate t\n",
      " Iter:49 - Batch 2171/2178 - Loss:1.4980170032736613Ison away Kindly gince of a go! I say so soon brant then Kate's and se\n",
      " Iter:50 - Batch 2171/2178 - Loss:1.4581447688968243on away to King Henry! but we heart death is a sout the she hather, \n",
      " Iter:51 - Batch 2171/2178 - Loss:1.4652385564775825Is the perfect so say some on thinks to his father, you shall death o'\n",
      " Iter:52 - Batch 2171/2178 - Loss:1.4712668901052246Ison away faronation, daughter, for thee and all this, And hath sooth \n",
      " Iter:53 - Batch 2171/2178 - Loss:1.4516778390728253Ison away that speak, and my lord, though the sun, The shKing, and amb\n",
      " Iter:54 - Batch 2171/2178 - Loss:1.4624243245549267Is the shrist. Temp to sweecK.  MENENIUS: ThKing over be Kind his fath\n",
      " Iter:55 - Batch 2171/2178 - Loss:1.4444144834885435s the shrist.  RICHARD: And so call with his oKenger: Keaven, death t\n",
      " Iter:56 - Batch 2171/2178 - Loss:1.4370530376187476Ison to reques at make to you, sir.  LADY ANNE: When you bid to see Th\n",
      " Iter:57 - Batch 2171/2178 - Loss:1.4131228087237966Is the Tapused.  ROMEO: Love the wind, and there.  POLINA: I thing is \n",
      " Iter:58 - Batch 2171/2178 - Loss:1.3994965833633102Is Kate courteer him well week of runation, thou hadst us a sweet we h\n",
      " Iter:59 - Batch 2171/2178 - Loss:1.3794016103139297sabKent and supKrit the King Her are there? Or all to bear And Kate'?\n",
      " Iter:60 - Batch 2171/2178 - Loss:1.3736492065102501Knation.  SKIN GHOwBMan, therefore-plernations, I thing it as speak sh\n",
      " Iter:61 - Batch 2171/2178 - Loss:1.3807844034807045asable shall death the earth to meet him one and we contrament days of\n",
      " Iter:62 - Batch 2171/2178 - Loss:1.3707077384080433InapKreen is ste ance and with her Kend. KING RICHARD III: End, sorrow\n",
      " Iter:63 - Batch 2171/2178 - Loss:1.3456647321111699asace and where less tale in that King Help, give the shrist. Hendy.  \n",
      " Iter:64 - Batch 2171/2178 - Loss:1.3365555270219092Is the Trems!  MENENIUS: Thou proces, pence Will asidest to relling ev\n",
      " Iter:65 - Batch 2171/2178 - Loss:1.3420047272553526IsKabous stee in the sire the watch, and the rued with me: I see, all \n",
      " Iter:66 - Batch 2171/2178 - Loss:1.3406465899752914Is the King Here cousin feerbreatest Kate'' e orance on thy woman-- I \n",
      " Iter:67 - Batch 2171/2178 - Loss:1.3283340411798241It fellow like a caused of rue.  DUKE VINCENTIO: 'Mark of the short, T\n",
      " Iter:68 - Batch 2171/2178 - Loss:1.3150913875796655t but we have like me, breas-- I shall Kate's shall we is banish'd of\n",
      " Iter:69 - Batch 2171/2178 - Loss:1.3072288985471272IKite our seKrid t. From the rued say so soon my lord, thou wictors of\n",
      " Iter:70 - Batch 2171/2178 - Loss:1.3089917304934864asable found Gement with me: I will death of you Kate,King King Henry \n",
      " Iter:71 - Batch 2171/2178 - Loss:1.3029946047158973asable doubt.  WARWICK: KGoo him cutcless of fortunes, and there.  FLO\n",
      " Iter:72 - Batch 2171/2178 - Loss:1.2799837481731808Manded and my read?  Provost: And, I have misd. Selfs Oxe the supKent \n",
      " Iter:73 - Batch 2171/2178 - Loss:1.2639540718669544Myself aKing of King Henry! beseect, and soKt deKent no gentle Kind hi\n",
      " Iter:74 - Batch 2171/2178 - Loss:1.2615892209852317asabel, and there.  PKING EDWARD IV: WelKing: I shall we of GKorant do\n",
      " Iter:75 - Batch 2171/2178 - Loss:1.2443877944075342asabel, That he is God, I have to be him well.  ESCALUS: And so will b\n",
      " Iter:76 - Batch 2171/2178 - Loss:1.2431333057636227Ison a procks It issue, and my daughter KingK'S humbKroats, where to a\n",
      " Iter:77 - Batch 2171/2178 - Loss:1.2351541247922596Ison away! 3 KING RICHARD II: Mend unlence.  ROMEO: Love the king me, \n",
      " Iter:78 - Batch 2171/2178 - Loss:1.2357616011058787sames, there best are there? We cannot mend and be content to be holl\n",
      " Iter:79 - Batch 2171/2178 - Loss:1.2363462433867265asabel, where is a sweet we Kingly sweet we Kingly sweet we heart, 'To\n",
      " Iter:80 - Batch 2171/2178 - Loss:1.2153863553272535sabel, where's their both is the soon my lord, though their adKing as\n",
      " Iter:81 - Batch 2171/2178 - Loss:1.2110093829067652asable so but we mernna--Ow, the sighs so a King Hendsome in Kind Her \n",
      " Iter:82 - Batch 2171/2178 - Loss:1.2064920309591449All: For a place.  PETRUCHIO: Sir, and I'll helps I would be ill.  DU\n",
      " Iter:83 - Batch 2171/2178 - Loss:1.2068394199169965sabel, where the coverly Kinglat and say some is the serve. What! sho\n",
      " Iter:84 - Batch 2171/2178 - Loss:1.1958911412873614Ison a KInd our set down rigg'e no own awart Let then cousin thyself. \n",
      " Iter:85 - Batch 2171/2178 - Loss:1.1926815712968766Ison a his forth will Kaintral him one, And Kate, Whild me to meet hKi\n",
      " Iter:86 - Batch 2171/2178 - Loss:1.1809289239984986asable death the earth, and there.  FLORIZCORIOLANUS: I thing ever I a\n",
      " Iter:87 - Batch 2171/2178 - Loss:1.1834082299127031asable death to reports I would be confiden errow men, year then, I'll\n",
      " Iter:88 - Batch 2171/2178 - Loss:1.1823227693892697Ison a son, Of all the rough to meed, whither stemp and sweet bKing de\n",
      " Iter:89 - Batch 2171/2178 - Loss:1.1695211810177495sable but reatKent no knaves, Kate courtes, how not a malmboutKers of\n",
      " Iter:90 - Batch 2171/2178 - Loss:1.1681644771763476asable son it is foreive bands, I say so a there?  KING EDWARD IV: Bri\n",
      " Iter:91 - Batch 2171/2178 - Loss:1.1645676142559636asable for their blaughter waKent dead.  HORTER: And so wine, the Aume\n",
      " Iter:92 - Batch 2171/2178 - Loss:1.1654879081769551asable son in his grieve the dnst!LLY:  Volcaid awakes.  LADY ANNE: Wh\n",
      " Iter:93 - Batch 2171/2178 - Loss:1.1610612582978435asable son, do not rage you, Kate courted, as see with his tongue of a\n",
      " Iter:94 - Batch 2171/2178 - Loss:1.1503464181457998asabed and so so say some is the seKroe: Kine.  PETRUCHIO: I stand don\n",
      " Iter:95 - Batch 2171/2178 - Loss:1.1454274464929919asable son?  CORIOLANUS: I think there: Yet he comes and words, But I \n",
      " Iter:96 - Batch 2171/2178 - Loss:1.1416938993944592asabsed when I am good to reports there be behim of This chird, the no\n",
      " Iter:97 - Batch 2171/2178 - Loss:1.1419980364289446Kall's more but on the is, into his forth the sun plainly.  LADY ANNE:\n",
      " Iter:98 - Batch 2171/2178 - Loss:1.1415029556933013sa, that speak show the wKilK'd our hosed and see with my bear the su\n",
      " Iter:99 - Batch 2171/2178 - Loss:1.1379275317102935Ison a dange they are not a man.  DUKE OF ELIZABETH: So we doth not ma\n"
     ]
    }
   ],
   "source": [
    "train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter:0 - Batch 2171/2178 - Loss:1.1285022991024116Kall's death, Which shoes as so it keeper: a man with Kingl wKill so o\n",
      " Iter:1 - Batch 2171/2178 - Loss:1.1301244746543011son a dreat, Is the coving drained so stemp, that love this adh danKs\n",
      " Iter:2 - Batch 2171/2178 - Loss:1.1334081258030045Is the King LooKINGHAM: Mends?  MENENIUS: Yes, comes are that saKin of\n",
      " Iter:3 - Batch 2171/2178 - Loss:1.1324357430859957son a gold to be much deliver sticks, thus in the pardo--bain and suc\n",
      " Iter:4 - Batch 2171/2178 - Loss:1.1295060289924141Ison a thousand GentleKer a dange on the sight!  AUTOLYCUS: I think th\n",
      " Iter:5 - Batch 2171/2178 - Loss:1.1266929125182459Ison a gold to be hollow me the sun more but onKth, and the wates, wit\n",
      " Iter:6 - Batch 2171/2178 - Loss:1.1296119457457334sKal lata, how notK?  Secvise to KateKing may no longs the plucking d\n",
      " Iter:7 - Batch 2171/2178 - Loss:1.1345663919877764Ison a thouses, and the wates, Where is a sweet we haste yet! he shall\n",
      " Iter:8 - Batch 2171/2178 - Loss:1.1283683976521541asable death tKe up as speak a sweet we haste yet withKe Ke of the eat\n",
      " Iter:9 - Batch 2171/2178 - Loss:1.1249879110067587Isomety good to his farKI ThKing as speak a cakes, sir; and I,-  BENVO\n",
      " Iter:10 - Batch 2171/2178 - Loss:1.1200431956066697Isabel, whild say some it cause in the Time of ciuself, All at home: W\n",
      " Iter:11 - Batch 2171/2178 - Loss:1.1210111772625206asable stemps shall we on, and till this against thy creet, you shall \n",
      " Iter:12 - Batch 2171/2178 - Loss:1.1209654480565785Is the plucking, we son, I think there: BKing no ther, that love him w\n",
      " Iter:13 - Batch 2171/2178 - Loss:1.1222232736084527sa, him one, Is thy King Loocon mine hath book not to requesty: of a \n",
      " Iter:14 - Batch 2171/2178 - Loss:1.1110095499677244IsabsTeK.  MENENIUS: Alas, and I,- Kind Henry what so any or Kingland?\n",
      " Iter:15 - Batch 2171/2178 - Loss:1.1075051125072897asa, him one, Is the perfect; And sKoward, I came a man safted how can\n",
      " Iter:16 - Batch 2171/2178 - Loss:1.1005520366374162Ison a dange theyKing dKe: WooKINGHRICHORD: Ay, to helps sent no Kate,\n",
      " Iter:17 - Batch 2171/2178 - Loss:1.1054348211718385son a gold, thusbapurs, good reKroe, and there.  PETRUCHIO: Sir, and \n",
      " Iter:18 - Batch 2171/2178 - Loss:1.1042367101217718Is the King LiKn Edward and it seKs, the profaness is instray some is \n",
      " Iter:19 - Batch 2171/2178 - Loss:1.0991513674419482IsKal lataged so a partune.  PETRUCHIO: Sir, where's the rough doom th\n",
      " Iter:20 - Batch 2171/2178 - Loss:1.0968638662247517Isan, nor and comforts, and not a man. For Claudio.  BAPTISTA: By lose\n",
      " Iter:21 - Batch 2171/2178 - Loss:1.0952637258336109Kall'd his father, you'r charge Our are as speKcalls Kaxe I starls, an\n",
      " Iter:22 - Batch 2171/2178 - Loss:1.0949434875375008Isabsed meKing stoant Gemngbraces, welcome and see, all resolt again o\n",
      " Iter:23 - Batch 2171/2178 - Loss:1.0977749164351252Isar.  ESCALUS: Any, I thing ever Kere I sent in our soldies with the \n",
      " Iter:24 - Batch 2171/2178 - Loss:1.0918043199167236Isabs.  First Murilied make a clift diein shortsols: That thou werK fe\n",
      " Iter:25 - Batch 2171/2178 - Loss:1.0957130752038562asable stempt, that fave the Timples have I may the honour! I came a m\n",
      " Iter:26 - Batch 2171/2178 - Loss:1.1032804013889963sabsed by Kind Halrs with me: Of a footion better hath bKing deed you\n",
      " Iter:27 - Batch 2171/2178 - Loss:1.1027207556900228asable stemp to be are sKeect dark, Attensice, nothing: You a loyd?  A\n",
      " Iter:28 - Batch 2171/2178 - Loss:1.0964030979421306All: Ke proces to requesty deed for to rejought him chargest straight\n",
      " Iter:29 - Batch 2171/2178 - Loss:1.0907668884887303KAll: When hath been there.  PAULENDA: Both, add then all thee lors st\n",
      " Iter:30 - Batch 2171/2178 - Loss:1.0855672816327393assently.  QUEEN ELIZABETH: What's with the dir.  SAMPOLIO: Why, some \n",
      " Iter:31 - Batch 2171/2178 - Loss:1.0856029476732014asst the coving drained so soon married with me: Of a footment.  HENRY\n",
      " Iter:32 - Batch 2171/2178 - Loss:1.0912871107108528sst the cKitter weKm me and my lord; while the sun thy jodged aKing s\n",
      " Iter:33 - Batch 2171/2178 - Loss:1.0999747741576997asst thee loronce of caKine.  POLINGORINCELUS: Thou prosdless is the f\n",
      " Iter:34 - Batch 2171/2178 - Loss:1.0931327332976983asleft fing death, In pristy day shonest sKIll part noKing Gud the wat\n",
      " Iter:35 - Batch 2171/2178 - Loss:1.0969770463796118asst thee loronce of ive with me: Of a footment no one hold unle is ba\n",
      " Iter:36 - Batch 2171/2178 - Loss:1.0907501683822051sabsed by your eyes,--is have I make our hosed with milling Kate be m\n",
      " Iter:37 - Batch 2171/2178 - Loss:1.0898956708670169asst thee loronce of ive our sent in the phopertia, rewkbitted en, yet\n",
      " Iter:38 - Batch 2171/2178 - Loss:1.0832333700852186asst the widow, perjust; And such am enterKrous are but we King death,\n",
      " Iter:39 - Batch 2171/2178 - Loss:1.0797696707722613asa, that say will not a plage thee losom, that best is necessaris, fa\n",
      " Iter:40 - Batch 2171/2178 - Loss:1.0806493361772354asleft deliest of your children earth aughter of their affecter! I spe\n",
      " Iter:41 - Batch 2171/2178 - Loss:1.0795836261014322asst thee lors from my cloat for thee lors in thy tent; then foKl deli\n",
      " Iter:42 - Batch 2171/2178 - Loss:1.0784465725239172Isar.  ESCALUS: And I,--not a malk's daughter King him one, I darge Th\n",
      " Iter:43 - Batch 2171/2178 - Loss:1.0809838124611333sar.  ENVt: When hange thee lord it known Drould not blinds they are \n",
      " Iter:44 - Batch 2171/2178 - Loss:1.0836894856821995asa, her lapr. This is that, and it should you have I mKove?  Second G\n",
      " Iter:45 - Batch 2171/2178 - Loss:1.0834327164268593asa, her lapriest; And there.  First Murilief to be holy masterprought\n",
      " Iter:46 - Batch 2171/2178 - Loss:1.0796808203609163asa, this sort a maltur Before the stold it pKint from my clorious ste\n",
      " Iter:47 - Batch 2171/2178 - Loss:1.0798065036950446asa, that save to me and my lord; whils, I senter, give the more subba\n",
      " Iter:48 - Batch 2171/2178 - Loss:1.0791401843807247asst thee lord it known Drour and stoopither have I Ka, fair uprest st\n",
      " Iter:49 - Batch 2171/2178 - Loss:1.0808706853682184asst thee lord it known Drould not blinds they are not make Warwick's \n",
      " Iter:50 - Batch 2171/2178 - Loss:1.0811089513844139asst thee lord it known Droulded oKd resu it us bad And it should swee\n",
      " Iter:51 - Batch 2171/2178 - Loss:1.0774118203847933asst thee lord it sorrow, perjoy! slain; And sup thy love the widows o\n",
      " Iter:52 - Batch 2171/2178 - Loss:1.0752548167242755Kword it to helt, I Kind it, and I'll me and my lord; whKing how it in\n",
      " Iter:53 - Batch 2171/2178 - Loss:1.0778836865983512asKrit Where King Hellow devill'st say some it can Kate be me, and the\n",
      " Iter:54 - Batch 2171/2178 - Loss:1.0727334462346397sabsed worKer hath been about sorring, Ilas! With the sun, That shone\n",
      " Iter:55 - Batch 2171/2178 - Loss:1.0753987571770964IsaK, Kate, I senter, give the men, Which worth will I beseech you, si\n",
      " Iter:56 - Batch 2171/2178 - Loss:1.0729828946153115words, and the King dreaven, lefe, 'I well, air?  QUEEN ELIZABETH: I \n",
      " Iter:57 - Batch 2171/2178 - Loss:1.0803059517296338asst the King Liry be ble; and IKell nKe; Say some a thousest it best \n",
      " Iter:58 - Batch 2171/2178 - Loss:1.0722702038414365Isabsed my sovereignt will so shall we may not Which denyer Kaider of \n",
      " Iter:59 - Batch 2171/2178 - Loss:1.0706057429903886Isabsed words with me: if we to rejess woe: on the shKings God save ha\n",
      " Iter:60 - Batch 2171/2178 - Loss:1.0718270774199292Isabsed words with me: is is welands; he is a sout To weep with the ho\n",
      " Iter:61 - Batch 2171/2178 - Loss:1.0750778174025368Isabsed words with the cloud off we dreat dreath am not.  GREMIO: I kn\n",
      " Iter:62 - Batch 2171/2178 - Loss:1.0699775569275394sabsed maKinester, That Kind Hanguabus, there back? First Senators of\n",
      " Iter:63 - Batch 2171/2178 - Loss:1.0698468109492998asst thee lord it known Dro no lords, to-mouth, and the worship now, a\n",
      " Iter:64 - Batch 2171/2178 - Loss:1.0690794942682575asable ot lime. What!  VARCLAUSO: Why, that I can; Go their both will \n",
      " Iter:65 - Batch 2171/2178 - Loss:1.0650501864744009asst the wishing day.  LEONTES: HKI Wall sorrow not make them one sent\n",
      " Iter:66 - Batch 2171/2178 - Loss:1.0706959343358429saK, the house, sweet bound; and I,-mendst, Which replain why, Kate m\n",
      " Iter:67 - Batch 2171/2178 - Loss:1.0717280907820708Isar. The King Help, sKa, for thee  First Myneming true a man Kind the\n",
      " Iter:68 - Batch 2171/2178 - Loss:1.0763906839111363sabsed my shoulder, Taith the she is ban Ilamperor: Sirswer, here cou\n",
      " Iter:69 - Batch 2171/2178 - Loss:1.0678226154844658Isabsed my swinkKing no lady's honour born; And I.  WARWICK: The lower\n",
      " Iter:70 - Batch 2171/2178 - Loss:1.0656784459563924King my child, put King Lord Haster, in plainly is the formn, Madgard \n",
      " Iter:71 - Batch 2171/2178 - Loss:1.0687846479870675sabsed wKeling many friends perKing dKe: Have you shall resile; and t\n",
      " Iter:72 - Batch 2171/2178 - Loss:1.0709723983981154Isar.  LADY ANNE: Glands; For, lords, with them one sently sitKe, how \n",
      " Iter:73 - Batch 2171/2178 - Loss:1.0703876696533419Isabsed wanns one King Hells of dest not. Sic, for I had rators!  And \n",
      " Iter:74 - Batch 2171/2178 - Loss:1.0654136269029055Isabsed words that shall we meem King Lord Haster, in plaKentio.  Voma\n",
      " Iter:75 - Batch 2171/2178 - Loss:1.0652201351612323Isabsed my should not being put the sun a part; they shall be daring K\n",
      " Iter:76 - Batch 2171/2178 - Loss:1.0666491490319876Isabsed my swink, king and supht? The not a man. Forth the sun this is\n",
      " Iter:77 - Batch 2171/2178 - Loss:1.0644166861525865Isabsed my swinkKect King Her aKing more be content to't a place of it\n",
      " Iter:78 - Batch 2171/2178 - Loss:1.0675004900386117Isabsed words with milest it, and I can black of your diferwand God, h\n",
      " Iter:79 - Batch 2171/2178 - Loss:1.0640720953836459Isabsed my cloat old, I sented Kate the honour May ban shall week them\n",
      " Iter:80 - Batch 2171/2178 - Loss:1.0656642127289564sabsed my sovereignt with me: is is welKing dares that say will she s\n",
      " Iter:81 - Batch 2171/2178 - Loss:1.0676557701053009Isabsed, with me your Kate, but about is oKe her hend; And Kate, but K\n",
      " Iter:82 - Batch 2171/2178 - Loss:1.0646349305762595sst the cart For Claiderer, where't theK and felKing inly say show my\n",
      " Iter:83 - Batch 2171/2178 - Loss:1.0669322373586565asst the cart For you begold, go you speak a man.  DUKE OF AUMIO: Kate\n",
      " Iter:84 - Batch 2171/2178 - Loss:1.0713371426477298Isabsed my sovereignt will begovedKing spurrow, perjude them one sente\n",
      " Iter:85 - Batch 2171/2178 - Loss:1.0669242675850287sabsed my swint shall be Kater? or not rigg Romeo sir, Rign I to the \n",
      " Iter:86 - Batch 2171/2178 - Loss:1.0636534377630857sabsed my shoulder, This most prove-moct,Kersed my clifther: How it i\n",
      " Iter:87 - Batch 2171/2178 - Loss:1.0648961272568338Isabsed my cloat Kate unth man deed him well.  KING RICHARD III: Even \n",
      " Iter:88 - Batch 2171/2178 - Loss:1.0691784601194869Isabsed my shoulderKing him on WKeldet King Henry's shall we meed you \n",
      " Iter:89 - Batch 2171/2178 - Loss:1.0714984274801373Isabsed my lord.  LEONTES: Heavy shall we do me Which revenrell'st min\n",
      " Iter:90 - Batch 2171/2178 - Loss:1.0664275587568195Isabsed my cloat of my brother of my poor, Kate, whereoping to me insu\n",
      " Iter:91 - Batch 2171/2178 - Loss:1.0623637167845324Isabsed my swint subjected for the worsh-liKe receive me friend their \n",
      " Iter:92 - Batch 2171/2178 - Loss:1.0644822116093935Isabsed my swintosK hence with me: And in the love him chargest him an\n",
      " Iter:93 - Batch 2171/2178 - Loss:1.0640331594242516absed my swintow: Romeo sir, advip: And, I have like a pleast.  POLI\n",
      " Iter:94 - Batch 2171/2178 - Loss:1.0633374257691006Isabsed my swintow: Roy, Kate King Lord Haster, in pleasure; Contempot\n",
      " Iter:95 - Batch 2171/2178 - Loss:1.0619023092589341Isabsed my will Kan thy lord, this is no more.  CLAUDIO: There's a pla\n",
      " Iter:96 - Batch 2171/2178 - Loss:1.0624418488206977Isabsed, with me your dif a make a plage thee lord will I enmits; thus\n",
      " Iter:97 - Batch 2171/2178 - Loss:1.0620723506780902Isabs't thou Kinglatcess is charist: And is that, come upon thy love, \n",
      " Iter:98 - Batch 2171/2178 - Loss:1.0602846177027632Isabsed wor that say will so shall I Kingly gift up the worsht you sha\n",
      " Iter:99 - Batch 2171/2178 - Loss:1.0612567131937721Isabsed, within to me instraughter of their aused with King of him.  I\n"
     ]
    }
   ],
   "source": [
    "train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isabs' thy bear the windows of the more shall week or not into my fellow of me sway;\n",
      "Unto the worsht ambebertian in the shrifther, one conteK;\n",
      "And worKly?\n",
      "\n",
      "PROSPERO:\n",
      "Thou plage.\n",
      "\n",
      "FLORY VI:\n",
      "BUUT: Wass with my your diferly.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Brief; subject, all the butan's shall we meet a footment not.\n",
      "\n",
      "LEONTES:\n",
      "KGARETHARINA:\n",
      "Why, sir,\n",
      "For whats\n",
      "And I could be so this life, as she comes and with the menire?\n",
      "\n",
      "ROMEO:\n",
      "What he does, with my your lordship being put the share counsel worthits the rough day.\n",
      "\n",
      "LKCLARD:\n",
      "Ay, we darly stir now, awayKrs' ere you here, dead\n",
      "The more slave to sweet brKill is inKeer with the menir.\n",
      "\n",
      "CATESBY:\n",
      "My lord, I beseech you, sir: but it swirt we do me instraughter of the earth, and thou couldst,\n",
      "Which delies you lief?\n",
      "\n",
      "LADY ANNE:\n",
      "Why, sorrabue of the more shall we meed. And, for the worK;\n",
      "And sweet bosf thy love, how cannot a part; this is that, come unth might husband God rationceard.\n",
      "\n",
      "RICHARD:\n",
      "An, what hKing have I make a process the buting in the shrifther, one cKoul lation:\n",
      "What, sir. FixiKe\n",
      "Than all thing in this there,\n",
      "pKint, and thy thind,\n",
      "I say shoopice my will infection of thy lord, that laKent in the shrKe: he that said as dest her point GeoKd in the lord,\n",
      "That Kasy someth with mKity\n",
      "Farewell. HerKrants turn thou canscon'd mine holy place is swintyr froward,\n",
      "Some settent you this?\n",
      "\n",
      "PRINCE:\n",
      "Thy to the more shall we meed. And, for the worshim more course,\n",
      "Where our hold unle sitch him.\n",
      "\n",
      "ILANDA:\n",
      "HeavyK.\n",
      "\n",
      "Yea,' the sight,\n",
      "And on the Kind on that bandy the noble thy women was the clifther in the more Kill wKeKr still she sayill:\n",
      "King; I speec more beyond? why shoulder of the easte our mean it is\n",
      "KingKenKed bright swell we dKink o'er a draind.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Brief;\n",
      "Mine faces from my palar?\n",
      "\n",
      "FLORIZIK:\n",
      "Hence, at the noble corruping to rejoure?\n",
      "Kine.\n",
      "\n",
      "First Mintagues and I came to interprehs, that I could be some on the shrift, true but thy lossectKe:\n",
      "If you speak.\n",
      "\n",
      "YORK:\n",
      "How now, part you shall resK-ly show\n",
      "King and daughter, yet you,K--K\n"
     ]
    }
   ],
   "source": [
    "print(generate_sample(n=2000, init_char='\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

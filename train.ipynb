{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 1: Import Libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 2: Define the Custom Tensor Class\n",
    "class NeuralTensor:\n",
    "    \"\"\"\n",
    "    A custom tensor class that supports automatic differentiation.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, requires_grad=False, creators=None, op_name=None, tensor_id=None):\n",
    "        self.data = np.array(data)\n",
    "        self.requires_grad = requires_grad\n",
    "        self.gradient = None\n",
    "        self.tensor_id = np.random.randint(0, 100000) if tensor_id is None else tensor_id\n",
    "        self.creators = creators\n",
    "        self.op_name = op_name\n",
    "        self.children = {}\n",
    "\n",
    "        if creators is not None:\n",
    "            for c in creators:\n",
    "                if self.tensor_id not in c.children:\n",
    "                    c.children[self.tensor_id] = 1\n",
    "                else:\n",
    "                    c.children[self.tensor_id] += 1\n",
    "\n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "        if self.requires_grad:\n",
    "            if grad is None:\n",
    "                grad = NeuralTensor(np.ones_like(self.data))\n",
    "\n",
    "            if grad_origin is not None:\n",
    "                if self.children[grad_origin.tensor_id] == 0:\n",
    "                    raise Exception(\"Cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.tensor_id] -= 1\n",
    "\n",
    "            if self.gradient is None:\n",
    "                self.gradient = grad\n",
    "            else:\n",
    "                self.gradient += grad\n",
    "\n",
    "            if self.creators is not None and (self._all_children_grads_accounted_for() or grad_origin is None):\n",
    "                if self.op_name == \"add\":\n",
    "                    self.creators[0].backward(self.gradient, self)\n",
    "                    self.creators[1].backward(self.gradient, self)\n",
    "                # Additional operations like 'sub', 'mul' etc. can be added here\n",
    "\n",
    "    def _all_children_grads_accounted_for(self):\n",
    "        for id, cnt in self.children.items():\n",
    "            if cnt != 0:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    # Define other tensor operations (__add__, __sub__, __mul__, etc.) here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 3: Define Neural Network Layers\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    Base class for all layers in the network.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Initialize the list of parameters for the layer\n",
    "        self.parameters = []\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Computes the forward pass of the layer.\n",
    "        :param input: Input data to the layer\n",
    "        :return: Layer output\n",
    "        \"\"\"\n",
    "        # To be implemented by subclasses\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, grad):\n",
    "        \"\"\"\n",
    "        Computes the backward pass of the layer.\n",
    "        :param grad: Gradient of the loss with respect to the output of the layer\n",
    "        :return: Gradient of the loss with respect to the input of the layer\n",
    "        \"\"\"\n",
    "        # To be implemented by subclasses\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"\n",
    "        Returns the parameters of the layer.\n",
    "        :return: List of parameters\n",
    "        \"\"\"\n",
    "        return self.parameters\n",
    "\n",
    "\n",
    "class LinearLayer(Layer):\n",
    "    \"\"\"\n",
    "    A fully connected neural network layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        \"\"\"\n",
    "        Initializes weights and biases for the linear layer.\n",
    "        :param n_inputs: Number of input features.\n",
    "        :param n_outputs: Number of output features.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weights = NeuralTensor(np.random.randn(n_inputs, n_outputs) * np.sqrt(2. / n_inputs), requires_grad=True)\n",
    "        self.bias = NeuralTensor(np.zeros(n_outputs), requires_grad=True)\n",
    "        self.parameters.append(self.weights)\n",
    "        self.parameters.append(self.bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward pass of the linear layer.\n",
    "        :param input: Input tensor.\n",
    "        :return: Output tensor of the linear transformation.\n",
    "        \"\"\"\n",
    "        return input.mm(self.weights) + self.bias\n",
    "\n",
    "    def backward(self, grad):\n",
    "        \"\"\"\n",
    "        Backward pass of the linear layer is not explicitly defined here,\n",
    "        as the NeuralTensor class handles automatic differentiation.\n",
    "        \"\"\"\n",
    "        pass  # The backward pass is handled automatically by the NeuralTensor class.\n",
    "\n",
    "\n",
    "class SigmoidLayer(Layer):\n",
    "    \"\"\"\n",
    "    A layer that applies the sigmoid activation function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward pass for sigmoid activation: sigmoid(x) = 1 / (1 + exp(-x))\n",
    "        \n",
    "        :param input: Input tensor for the layer.\n",
    "        :return: Output tensor after applying sigmoid activation.\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        return 1 / (1 + np.exp(-input))\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass for the sigmoid activation.\n",
    "        \n",
    "        :param grad_output: Gradient of the loss function with respect to the output of this layer.\n",
    "        :return: Gradient of the loss function with respect to the input of this layer.\n",
    "        \"\"\"\n",
    "        sigmoid = 1 / (1 + np.exp(-self.input))\n",
    "        return grad_output * sigmoid * (1 - sigmoid)\n",
    "\n",
    "\n",
    "class TanhLayer(Layer):\n",
    "    \"\"\"\n",
    "    A layer that applies the tanh activation function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Computes the forward pass using the tanh function.\n",
    "\n",
    "        :param input: Input tensor for the layer.\n",
    "        :return: Output tensor after applying the tanh function.\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        return np.tanh(input)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Computes the backward pass of the tanh function.\n",
    "\n",
    "        :param grad_output: Gradient of the loss function with respect to the output of this layer.\n",
    "        :return: Gradient of the loss function with respect to the input of this layer.\n",
    "        \"\"\"\n",
    "        tanh_grad = 1 - np.tanh(self.input) ** 2\n",
    "        return grad_output * tanh_grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 4: Define RNN Layer\n",
    "class RNNLayer(Layer):\n",
    "    \"\"\"\n",
    "    A layer in a Recurrent Neural Network.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, activation='sigmoid'):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.activation = SigmoidLayer() if activation == 'sigmoid' else TanhLayer()\n",
    "\n",
    "        self.input_hidden_layer = LinearLayer(input_size, hidden_size)\n",
    "        self.hidden_hidden_layer = LinearLayer(hidden_size, hidden_size)\n",
    "        self.hidden_output_layer = LinearLayer(hidden_size, output_size)\n",
    "\n",
    "        self.parameters = (self.input_hidden_layer.get_parameters() +\n",
    "                           self.hidden_hidden_layer.get_parameters() +\n",
    "                           self.hidden_output_layer.get_parameters())\n",
    "\n",
    "    def forward(self, input_tensor, hidden_tensor):\n",
    "        from_prev_hidden = self.hidden_hidden_layer.forward(hidden_tensor)\n",
    "        combined = self.input_hidden_layer.forward(input_tensor) + from_prev_hidden\n",
    "        new_hidden = self.activation.forward(combined)\n",
    "        output = self.hidden_output_layer.forward(new_hidden)\n",
    "        return output, new_hidden\n",
    "\n",
    "    def init_hidden_state(self, batch_size=1):\n",
    "        return NeuralTensor(np.zeros((batch_size, self.hidden_size)), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 5: Define Additional Components\n",
    "class EmbeddingLayer(Layer):\n",
    "    \"\"\"\n",
    "    An embedding layer to map input indices to dense vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        \"\"\"\n",
    "        Initializes the EmbeddingLayer with random weights.\n",
    "\n",
    "        :param vocab_size: The size of the vocabulary.\n",
    "        :param embedding_dim: The dimensionality of the embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Initialize the embeddings\n",
    "        self.weights = Tensor((np.random.rand(vocab_size, embedding_dim) - 0.5) / embedding_dim, autograd=True)\n",
    "        self.parameters.append(self.weights)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward pass of the embedding layer. Maps input indices to embeddings.\n",
    "\n",
    "        :param input: A batch of indices with shape (batch_size,).\n",
    "        :return: The corresponding embeddings with shape (batch_size, embedding_dim).\n",
    "        \"\"\"\n",
    "        return self.weights.index_select(input)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass of the embedding layer. Updates gradients for embeddings.\n",
    "\n",
    "        :param grad_output: The gradient of the loss with respect to the output of the embedding layer.\n",
    "        \"\"\"\n",
    "        self.weights.backward(grad_output)\n",
    "\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    \"\"\"\n",
    "    A class to compute the cross entropy loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        Forward pass for computing the cross entropy loss.\n",
    "\n",
    "        :param input: Predictions from the model, shape (batch_size, num_classes).\n",
    "        :param target: Ground truth labels, shape (batch_size,).\n",
    "        :return: The computed cross entropy loss.\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        self.softmax_output = self._softmax(input)\n",
    "        self.log_likelihood = -np.log(self.softmax_output[range(target.shape[0]), target])\n",
    "        loss = np.sum(self.log_likelihood) / input.shape[0]\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Backward pass for computing the gradient of the cross entropy loss \n",
    "        with respect to the input.\n",
    "\n",
    "        :return: The gradients with respect to the input.\n",
    "        \"\"\"\n",
    "        dx = self.softmax_output\n",
    "        dx[range(self.target.shape[0]), self.target] -= 1\n",
    "        dx = dx / self.target.shape[0]\n",
    "        return dx\n",
    "\n",
    "    def _softmax(self, x):\n",
    "        \"\"\"\n",
    "        Private method to compute softmax values for each set of scores in x.\n",
    "\n",
    "        :param x: Input array.\n",
    "        :return: Softmax output array.\n",
    "        \"\"\"\n",
    "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent (SGD) optimizer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, parameters, alpha=0.01):\n",
    "        \"\"\"\n",
    "        Initializes the SGD optimizer.\n",
    "\n",
    "        :param parameters: A list of parameters to optimize.\n",
    "        :param alpha: The learning rate.\n",
    "        \"\"\"\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def zero(self):\n",
    "        \"\"\"\n",
    "        Resets the gradients of all parameters to zero.\n",
    "        \"\"\"\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "\n",
    "    def step(self, zero=True):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "\n",
    "        :param zero: If True, resets gradients to zero after the step.\n",
    "        \"\"\"\n",
    "        for p in self.parameters:\n",
    "            p.data -= p.grad.data * self.alpha\n",
    "            \n",
    "            if zero:\n",
    "                p.grad.data *= 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 6: Define Preprocessing Function\n",
    "def preprocess_shakespeare_text(file_path):\n",
    "    \"\"\"\n",
    "    Preprocesses the Shakespeare text dataset.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        raw_text = file.read()\n",
    "\n",
    "    vocab = list(set(raw_text))\n",
    "    char_to_index = {char: i for i, char in enumerate(vocab)}\n",
    "    index_to_char = {i: char for i, char in enumerate(vocab)}\n",
    "    indexed_data = np.array([char_to_index[char] for char in raw_text])\n",
    "\n",
    "    return raw_text, vocab, char_to_index, index_to_char, indexed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 7: Define Training Function\n",
    "def train_rnn_model(model, data, epochs=10, batch_size=32, sequence_length=100, learning_rate=0.1):\n",
    "    \"\"\"\n",
    "    Trains the RNN model.\n",
    "    \"\"\"\n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = SGD(model.get_parameters(), alpha=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        n_loss = 0\n",
    "\n",
    "        hidden = model.init_hidden_state(batch_size)\n",
    "        for batch_i in range(0, data.size(0) - sequence_length, sequence_length):\n",
    "            optimizer.zero()\n",
    "\n",
    "            hidden = NeuralTensor(hidden.data, requires_grad=True)\n",
    "            loss = None\n",
    "\n",
    "            for t in range(sequence_length):\n",
    "                input = NeuralTensor(data[batch_i:batch_i + sequence_length], requires_grad=True)\n",
    "                rnn_input = embed.forward(input=input)\n",
    "                output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "\n",
    "                target = NeuralTensor(data[batch_i + 1:batch_i + sequence_length + 1], requires_grad=True)\n",
    "                batch_loss = criterion.forward(output, target)\n",
    "                if loss is None:\n",
    "                    loss = batch_loss\n",
    "                else:\n",
    "                    loss += batch_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.data\n",
    "\n",
    "        print(f'Epoch {epoch}, Loss: {total_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 8: Load Data and Initialize Model\n",
    "file_path = 'tinyshakespeare.txt'\n",
    "raw_text, vocab, char_to_index, index_to_char, indexed_data = preprocess_shakespeare_text(file_path)\n",
    "\n",
    "embed = EmbeddingLayer(vocab_size=len(vocab), dim=512)\n",
    "model = RNNLayer(input_size=512, hidden_size=512, output_size=len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 9: Train the Model\n",
    "train_rnn_model(model, indexed_data, epochs=40, batch_size=32, sequence_length=100, learning_rate=0.05)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
